{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Aula 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Install all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install stable-baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Import all dependencies and Create first Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:177: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 // Score 17.0\n",
      "Ep 1 // Score 31.0\n",
      "Ep 2 // Score 21.0\n",
      "Ep 3 // Score 18.0\n",
      "Ep 4 // Score 38.0\n",
      "Ep 5 // Score 16.0\n",
      "Ep 6 // Score 41.0\n",
      "Ep 7 // Score 20.0\n",
      "Ep 8 // Score 32.0\n",
      "Ep 9 // Score 34.0\n"
     ]
    }
   ],
   "source": [
    "# Create \"Hello World\" environment in RL\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Lets see environment \n",
    "episodes = 10\n",
    "for ep in range(episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    for step in range(100):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Ep {} // Score {}\".format(ep, score))\n",
    "env.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "LOG_PATH = os.path.join(\"logs\", \"PPO\")\n",
    "\n",
    "# Parallel environments\n",
    "env = make_vec_env(\"CartPole-v1\")\n",
    "\n",
    "model = PPO('MlpPolicy', env, tensorboard_log=LOG_PATH, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to logs\\PPO\\PPO_3\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.8     |\n",
      "|    ep_rew_mean     | 23.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 186      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.9        |\n",
      "|    ep_rew_mean          | 27.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 267         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009453945 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.0108     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.27        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 63.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34          |\n",
      "|    ep_rew_mean          | 34          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 311         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008836592 |\n",
      "|    clip_fraction        | 0.0499      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.0574      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 35.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 47          |\n",
      "|    ep_rew_mean          | 47          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 339         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009274963 |\n",
      "|    clip_fraction        | 0.0887      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.64       |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 51.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 62.5        |\n",
      "|    ep_rew_mean          | 62.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 362         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005177984 |\n",
      "|    clip_fraction        | 0.043       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.62       |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 62.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 79.2         |\n",
      "|    ep_rew_mean          | 79.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 377          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073717777 |\n",
      "|    clip_fraction        | 0.0798       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.599       |\n",
      "|    explained_variance   | 0.485        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.8         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0174      |\n",
      "|    value_loss           | 59.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 96.5         |\n",
      "|    ep_rew_mean          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 390          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071143964 |\n",
      "|    clip_fraction        | 0.0658       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.597       |\n",
      "|    explained_variance   | 0.618        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.9         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0137      |\n",
      "|    value_loss           | 52.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 113         |\n",
      "|    ep_rew_mean          | 113         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 399         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007518756 |\n",
      "|    clip_fraction        | 0.041       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.54        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00602    |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 131          |\n",
      "|    ep_rew_mean          | 131          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 405          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047623063 |\n",
      "|    clip_fraction        | 0.0163       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.572       |\n",
      "|    explained_variance   | 0.672        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.54         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00394     |\n",
      "|    value_loss           | 37.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 150         |\n",
      "|    ep_rew_mean          | 150         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 411         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003055601 |\n",
      "|    clip_fraction        | 0.0242      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.4        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00615    |\n",
      "|    value_loss           | 57          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 170         |\n",
      "|    ep_rew_mean          | 170         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 415         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005184812 |\n",
      "|    clip_fraction        | 0.0436      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.409       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00739    |\n",
      "|    value_loss           | 11.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 189         |\n",
      "|    ep_rew_mean          | 189         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 419         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009818915 |\n",
      "|    clip_fraction        | 0.0375      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.168       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.489       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00163    |\n",
      "|    value_loss           | 14.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 206          |\n",
      "|    ep_rew_mean          | 206          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048277834 |\n",
      "|    clip_fraction        | 0.0221       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.542       |\n",
      "|    explained_variance   | 0.326        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.439        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00344     |\n",
      "|    value_loss           | 4.54         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x18312dd5610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=25000)\n",
    "# model.learn(total_timesteps=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 // Score 500.0\n",
      "Ep 1 // Score 500.0\n",
      "Ep 2 // Score 500.0\n"
     ]
    }
   ],
   "source": [
    "# Create \"Hello World\" environment in RL\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Lets see environment \n",
    "episodes = 3\n",
    "for ep in range(episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done and score < 500:\n",
    "        # action = env.action_space.sample() # Random action\n",
    "        # Policy action\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Ep {} // Score {}\".format(ep, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
